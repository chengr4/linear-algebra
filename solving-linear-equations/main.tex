\documentclass[12pt,a4paper]{article}
\usepackage{amsfonts, amssymb, amsmath}
\usepackage{fullpage}
\usepackage{parskip} % skip a line instead of indenting
\usepackage{amsthm}
\usepackage{xcolor}

\newtheorem*{rem}{Remark}

\title{Solving Linear Equations}
\author{R4 Cheng}
\date{\today}

\newcommand{\Remark}[1]{
  \begin{rem}
    \color{cyan}
    #1
  \end{rem}
}

\begin{document}
\maketitle

\subsection*{Matrix Operations}

$
\begin{bmatrix}
  1 & 2 & -4 \\
  -2 & 3 & 1 \\
  4 & 1 & 2
\end{bmatrix}
\begin{bmatrix}
  x_1 & x_2 & x_3 \\
  y_1 & y_2 & y_3 \\
  z_1 & z_2 & z_3 
\end{bmatrix} = 
\begin{bmatrix}
  & \\
  \underline{c_1} & \underline{c_2} & \underline{c_3} \\
  &
\end{bmatrix}
$

$
\underline{c_2} = 
x_2
\begin{bmatrix}
  1 \\
  -2 \\
  4
\end{bmatrix} + 
y_2
\begin{bmatrix}
  2 \\
  3 \\
  1
\end{bmatrix} +
z_2
\begin{bmatrix}
  -4 \\
  1 \\
  2
\end{bmatrix}
$

$
\begin{bmatrix}
  a & b & c \\
\end{bmatrix}
\begin{bmatrix}
  1 & 2 & -4 \\
  -2 & 3 & 1 \\
  4 & 1 & 2
\end{bmatrix} = 
a
\begin{bmatrix}
  1 & 2 & -4 \\
\end{bmatrix} +
b
\begin{bmatrix}
  -2 & 3 & 1 \\
\end{bmatrix} +
c
\begin{bmatrix}
  4 & 1 & 2
\end{bmatrix}
$


\subsection*{Properties of Matrices}

$A(BC) = (AB)C$ (Associative law holds)

$AB \neq BA$ (Commutative law does not hold)

$C(A+B) = CA + CB$ or $(A+B)C = AC + BC$ (Distributive laws hold)

\Remark{We can change Guassian Elimination to Matrix multiplication}

\subsection*{Identity Matrix}
The identity matrix is a square matrix with ones on the main diagonal and zeros elsewhere. 
It is denoted by $I$ or $I_n$ for an $n \times n$ matrix.
$AI = IA = A$, for any $n \times n$ matrix $A$.

\[
I = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}
\]


\subsection*{Inverse Matrix}
The inverse of a square matrix $A$, denoted as $A^{-1}$, is a matrix such that $AA^{-1} = A^{-1}A = I$, where $I$ is the identity matrix. The inverse matrix can be found using the formula:

\[
A^{-1} = \frac{1}{{\text{det}(A)}} \cdot \text{adj}(A)
\]

where $\text{det}(A)$ is the determinant of matrix $A$ and $\text{adj}(A)$ is the adjugate of matrix $A$.

For example, to find the inverse of a $2 \times 2$ matrix:

\[
A = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\]

the inverse matrix is given by:

\[
A^{-1} = \frac{1}{{ad - bc}} \cdot \begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
\]

To find the inverse of a $3 \times 3$ matrix, you can use the formula:

\[
A^{-1} = \frac{1}{{\text{det}(A)}} \cdot \text{adj}(A)
\]

where $\text{adj}(A)$ is the adjugate of matrix $A$. The adjugate of a $3 \times 3$ matrix is given by:

\[
\text{adj}(A) = \begin{bmatrix}
A_{11} & A_{21} & A_{31} \\
A_{12} & A_{22} & A_{32} \\
A_{13} & A_{23} & A_{33}
\end{bmatrix}
\]

where $A_{ij}$ is the cofactor of element $a_{ij}$ in matrix $A$.

Note that not all matrices have an inverse. A matrix is invertible if and only if its determinant is non-zero.

\subsubsection*{Attributes}

\begin{itemize}
  \item It is unique.
  \item The inverse of $A^{-1}$ is $A$ itself.
\end{itemize}

\hrulefill

\textbf{Claim.} Suppose A is invertible. Then its inverse is unique.

\textbf{Proof.} Suppose $B$ and $C$ are both inverses of $A$.
Then $B = BI = B(AC) = (BA)C = IC = C$.

\Remark{left inverse = right inverse = inverse}

\textbf{Claim.} The inverse of $A^{-1}$ is $A$ itself.

\textbf{Proof.} $AA^{-1} = I$ and $A^{-1}A = I$.

\textbf{Claim.} If $A$ is invertible, then the one and only solution to $A\underline{x} = \underline{b}$ is $\underline{x} = A^{-1}\underline{b}$.

\textbf{Proof.} $A\underline{x} = \underline{b} \Rightarrow A^{-1}A\underline{x} = A^{-1}\underline{b} \Rightarrow \underline{x} = A^{-1}\underline{b}$.

\textbf{Claim.} Suppose there is a nonzero solution $\underline{x}$ to $A\underline{x} = \underline{0}$ (homogeneous equation).
Then $A$ is not invertible.

\textbf{Proof.} If $A$ is invertible, then $A^{-1}$ exists.
Then $A^{-1}A\underline{x} = A^{-1}\underline{0} \Rightarrow \underline{x} = \underline{0}$.

\textbf{Claim.} A diagonal matrix has an inverse provided no diagonal entries are zero.

\textbf{Proof.} \\
If

\[
A = \begin{bmatrix}
  d_1 & 0 & \cdots & 0 \\
  0 & d_2 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & d_n
\end{bmatrix}
\]

then

\[
A^{-1} = \begin{bmatrix}
  \frac{1}{d_1} & 0 & \cdots & 0 \\
  0 & \frac{1}{d_2} & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & \frac{1}{d_n}
\end{bmatrix}
\]

\textbf{Claim.} If $A$ and $B$ are invertible, then $AB$ is invertible and $(AB)^{-1} = B^{-1}A^{-1}$.

\textbf{Proof.} 

$(B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = B^{-1}IB = I$ \\
$(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AIA^{-1} = I$

\Remark{$(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$}

\subsection*{Gauss-Jordan Elimination}

Given $A$, we want to find its inverse $A^{-1}$. $AA^{-1} = I$

\[
A
\begin{bmatrix}
  \underline{col_1} & \underline{col_3} & \underline{col_3} \\
\end{bmatrix} = 
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{bmatrix} = 
\begin{bmatrix}
  \underline{e_1} & \underline{e_3} & \underline{e_3} \\
\end{bmatrix}
\]

$
A = 
\begin{bmatrix}
  2 & -1 & 0 \\ 
  -1 & 2 & -1 \\
  0 & -1 & 2
\end{bmatrix}
$

\[
\begin{bmatrix}
  \boxed{2} & -1 & 0 & \vdots & 1 & 0 & 0 \\
  -1 & 2 & -1 & \vdots & 0 & 1 & 0 \\
  0 & -1 & 2 & \vdots & 0 & 0 & 1
\end{bmatrix}
\]
$\Rightarrow$
\[
\begin{bmatrix}
  2 & -1 & 0 & \vdots & 1 & 0 & 0 \\
  0 & \boxed{\frac{3}{2}} & -1 & \vdots & \frac{1}{2} & 1 & 0 \\
  0 & -1 & 2 & \vdots & 0 & 0 & 1
\end{bmatrix}
\]
$\Rightarrow$
\[
\begin{bmatrix}
  2 & -1 & 0 & \vdots & 1 & 0 & 0 \\
  0 & \frac{3}{2} & -1 & \vdots & \frac{1}{2} & 1 & 0 \\
  0 & 0 & \boxed{\frac{4}{3}} & \vdots & \frac{1}{3} & \frac{2}{3} & 1
\end{bmatrix}
\]
Until here is Gauss \\
$\Rightarrow$
\[
\begin{bmatrix}
  2 & -1 & 0 & \vdots & 1 & 0 & 0 \\
  0 & \frac{3}{2} & 0 & \vdots & \frac{3}{4} & \frac{3}{2} & \frac{3}{4} \\
  0 & 0 & \boxed{\frac{4}{3}} & \vdots & \frac{1}{3} & \frac{2}{3} & 1
\end{bmatrix}
\]
$\Rightarrow$
\[
\begin{bmatrix}
  2 & 0 & 0 & \vdots & \frac{3}{2} & 1 & \frac{1}{2} \\
  0 & \boxed{\frac{3}{2}} & 0 & \vdots & \frac{3}{4} & \frac{3}{2} & \frac{3}{4} \\
  0 & 0 & \boxed{\frac{4}{3}} & \vdots & \frac{1}{3} & \frac{2}{3} & 1
\end{bmatrix}
\]
$\Rightarrow$
\[
\begin{bmatrix}
  1 & 0 & 0 & \vdots & \frac{3}{4} & \frac{1}{2} & \frac{1}{4} \\
  0 & 1 & 0 & \vdots & \frac{1}{2} & 1 & \frac{1}{2} \\
  0 & 0 & 1 & \vdots & \frac{1}{4} & \frac{1}{2} & \frac{3}{4}
\end{bmatrix}
\]
Until here is Jordan \\

$
col_1 = 
\begin{bmatrix}
  \frac{3}{4} \\
  \frac{1}{2} \\
  \frac{1}{4}
\end{bmatrix}
$, 
$
col_2 =
\begin{bmatrix}
  \frac{1}{2} \\
  1 \\
  \frac{1}{2}
\end{bmatrix}
$,
$
col_3 =
\begin{bmatrix}
  \frac{1}{4} \\
  \frac{1}{2} \\
  \frac{3}{4}
\end{bmatrix}
$,
$
A^{-1} =
\begin{bmatrix}
  \frac{3}{4} & \frac{1}{2} & \frac{1}{4} \\
  \frac{1}{2} & 1 & \frac{1}{2} \\
  \frac{1}{4} & \frac{1}{2} & \frac{3}{4}
\end{bmatrix}
$

\textbf{Claim.} A matrix is invertible if and only if (iff.) it is nonsingular.

\subsection*{Elimination = Factorization: $A = LU$}

TODO

\textbf{Claim.} If $A = L_1D_1U_1$ and $A = L_2D_2U_2$, where the $L$'s are lower-triangular with unit diagonal,
the $U$'s are upper-triangular with unit diagonal, and the $D$'s are diagonal matrices with no zeros on the diagonal,
then $L_1 = L_2$, $D_1 = D_2$, and $U_1 = U_2$.

\subsection*{One Square System $=$ Two Triangular Systems}

Benifit: ????

$A\underline{x} = \underline{b}$. Suppose elimination requires no row exchanges.

$A = LU \Rightarrow LU\underline{x} = \underline{b} \Rightarrow U\underline{x} = L^{-1}\underline{b} = \underline{c}$

We have $U\underline{x}$ where $L\underline{c} = \underline{b}$.

\begin{enumerate}
  \item Factor $A= LU$ by Gaussian eliminatio $\underline{c} = L^{-1}\underline{b}$.
  \item Solve $\underline{c}$ from $L\underline{c} = \underline{b}$ (forward elimination) and then solve $U\underline{x} = \underline{c}$ (backward elimination)
\end{enumerate}

E.g.

\[
\begin{bmatrix}
  2 & -1 & 0 \\
  4 & -6 & 0 \\
  -2 & 7 & 2
\end{bmatrix}
\begin{bmatrix}
  x \\
  y \\
  z
\end{bmatrix} = 
\begin{bmatrix}
  5 \\
  -2 \\
  9
\end{bmatrix}
\]
$$A\underline{x} = \underline{b}$$
$\Rightarrow$
\[
\begin{bmatrix}
  2 & 1 & 1 \\
  4 & -6 & 0 \\
  -2 & 7 & 2
\end{bmatrix} = 
\begin{bmatrix}
  1 & 0 & 0 \\
  2 & 1 & 0 \\
  -1 & -1 & 1
\end{bmatrix}
\begin{bmatrix}
  2 & 1 & 0 \\
  0 & -8 & -2 \\
  0 & 0 & 1
\end{bmatrix}
\]
$$A = LU$$
$\Rightarrow$
\[
\begin{bmatrix}
  1 & 0 & 0 \\
  2 & 1 & 0 \\
  -1 & -1 & 1
\end{bmatrix}
\begin{bmatrix}
  c_1 \\
  c_2 \\
  c_3
\end{bmatrix} =
\begin{bmatrix}
  5 \\
  -2 \\
  9
\end{bmatrix}
\]
$$L\underline{c} = \underline{b}$$

\[
\therefore
\underline{c} = 
\begin{bmatrix}
  c_1 \\
  c_2 \\
  c_3
\end{bmatrix} = 
\begin{bmatrix}
  5 \\
  -12 \\
  9
\end{bmatrix}
\]
\[
  \begin{bmatrix}
    2 & 1 & 1 \\
    0 & -8 & -2 \\
    0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y \\
    z
  \end{bmatrix} =
  \begin{bmatrix}
    5 \\
    -12 \\
    9
  \end{bmatrix}
\]
$$U\underline{x} = \underline{c}$$
\[
  \therefore
  \underline{x} = 
  \begin{bmatrix}
    x \\
    y \\
    z
  \end{bmatrix} = 
  \begin{bmatrix}
    1 \\
    1 \\
    2
  \end{bmatrix}
\]

\subsection*{Complexity of Elimination}
\subsubsection*{1.}
Solve $A\underline{x} = \underline{b}$

\[
  A = 
  \begin{bmatrix}
    a_{11} & a_{12} & \hdots & a_{1n} \\
    a_{21} & a_{22} & \hdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \hdots & a_{nn}
  \end{bmatrix} = 
  LU
\]

Gaussian Elimination:

1st stage: $n(n-1) \approx n^2$ (\# multiplications and additions) \\
2nd stage: $(n-1)^2$ \\
3rd stage: $(n-2)^2$ \\
$\vdots$ \\
$n^2 + (n-1)^2 + \cdots + 1^2 = \frac{1}{3}n(n+\frac{1}{2})(n+1) \approx \frac{n^3}{3}$

Solve $L\underline{c} = \underline{b}$

\[
  \begin{bmatrix}
    1 & 0 & \hdots & 0 \\
    val & 1 & \hdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    val & val & \hdots & 1
  \end{bmatrix}
  \begin{bmatrix}
    c_1 \\
    c_2 \\
    \vdots \\
    c_n
  \end{bmatrix} =
  \begin{bmatrix}
    b_1 \\
    b_2 \\
    \vdots \\
    b_n
  \end{bmatrix}
\]

$(n-1) + (n-2) + \cdots + 2 + 1 = \frac{n(n-1)}{2} \approx \frac{n^2}{2}$


\Remark{$b_1$ needs to be multiplied $(n-1)$ times, $b_2$ needs to be multiplied $(n-2)$ times}

Solve $U\underline{x} = \underline{c}$

\[
  \begin{bmatrix}
    pivot_1 & val & \cdots & val \\
    0 & p_2 & \cdots & val \\
    \cdots & \cdots & \ddots & \cdots \\
    0 & 0 & \cdots & p_n
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
  \end{bmatrix} =
  \begin{bmatrix}
    c_1 \\
    c_2 \\
    \vdots \\
    c_n
  \end{bmatrix}
\]

$1 + 2 + \hdots + n = \frac{n(n-1)}{2} \approx \frac{n^2}{2}$

Total: $\# = \frac{n^3}{3} + \frac{n^2}{2} + \frac{n^2}{2} \approx \frac{n^3}{3}$

\end{document}